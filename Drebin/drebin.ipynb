{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ff535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3870ed5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "#from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import f1_score as f1\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "import autokeras as ak\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from art.estimators.classification import LightGBMClassifier\n",
    "from art.attacks.evasion import ZooAttack\n",
    "from art.attacks.poisoning import PoisoningAttackBackdoor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69a1f9",
   "metadata": {},
   "source": [
    "# Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "025a9cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = 'data/drebin.csv'\n",
    "columns = list(pd.read_csv('data/dataset-features-categories.csv', header = None).iloc[:,0])\n",
    "# Importing the dataset\n",
    "Drebin_data = pd.read_csv(drebin_data_path, names = columns)\n",
    "\n",
    "X = Drebin_data.iloc[:,range(0,Drebin_data.shape[1]-1)].values\n",
    "y = Drebin_data.iloc[:, -1].values\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(y)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.astype(int), test_size = 0.3, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b4d1e",
   "metadata": {},
   "source": [
    "# Train-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8907912",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "SVM\n",
      "accuracy 0.9796054090002216\n",
      "---------------------------\n",
      "MLP\n",
      "accuracy 0.9895810241631567\n",
      "---------------------------\n",
      "XGB\n",
      "accuracy 0.9904677455109732\n",
      "---------------------------\n",
      "LGBM\n",
      "accuracy 0.9904677455109732\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "classifiers = {\"SVM\": SVC(kernel = 'linear', degree=3), \"MLP\": MLPClassifier(random_state=1, max_iter=300), \"XGB\": XGBClassifier(), \"LGBM\": LGBMClassifier()} \n",
    "\n",
    "for classifier_pair in classifiers.items():\n",
    "    print(\"---------------------------\")\n",
    "    print(classifier_pair[0])\n",
    "    classifier = classifier_pair[1]\n",
    "    classifier.fit(X_train, y_train)\n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    #compute accuracy_score\n",
    "    accuracy = acc(y_test, y_pred)\n",
    "    print('accuracy', accuracy)\n",
    "    \n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bd3355",
   "metadata": {},
   "source": [
    "# AutoKeras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a75a5aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 215)]             0         \n",
      "                                                                 \n",
      " multi_category_encoding (Mu  (None, 215)              0         \n",
      " ltiCategoryEncoding)                                            \n",
      "                                                                 \n",
      " normalization (Normalizatio  (None, 215)              431       \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               55296     \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                8224      \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      " classification_head_1 (Acti  (None, 1)                0         \n",
      " vation)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 63,984\n",
      "Trainable params: 63,553\n",
      "Non-trainable params: 431\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "saved_model=keras.models.load_model('structured_data_classifier/best_model', compile=True)\n",
    "print(saved_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "781b22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoKerasModel(path_best_model):\n",
    "    saved_model = keras.models.load_model(path_best_model, compile=True)\n",
    "    input_layer = Input(shape=(215,))\n",
    "    x = saved_model.layers[1](input_layer)\n",
    "    x = saved_model.layers[2](x)\n",
    "    x = Dense(units=256)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dense(units=32)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dense(units=1)(x)\n",
    "    x = saved_model.layers[-1](x)\n",
    "    new_model = Model(inputs=input_layer, outputs=x)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "555103cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "329/329 [==============================] - 5s 7ms/step - loss: 0.0933 - accuracy: 0.9643\n",
      "Epoch 2/15\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 0.0414 - accuracy: 0.9853\n",
      "Epoch 3/15\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 0.0297 - accuracy: 0.9905\n",
      "Epoch 4/15\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 0.0221 - accuracy: 0.9934\n",
      "Epoch 5/15\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 0.0177 - accuracy: 0.9943\n",
      "Epoch 6/15\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 0.0129 - accuracy: 0.9959\n",
      "Epoch 7/15\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 0.0120 - accuracy: 0.9971\n",
      "Epoch 8/15\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 0.0110 - accuracy: 0.9973\n",
      "Epoch 9/15\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 0.0130 - accuracy: 0.9962\n",
      "Epoch 10/15\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 0.0260 - accuracy: 0.9927\n",
      "Epoch 11/15\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 0.0128 - accuracy: 0.9966\n",
      "Epoch 12/15\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 0.0077 - accuracy: 0.9982\n",
      "Epoch 13/15\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 0.0066 - accuracy: 0.9987\n",
      "Epoch 14/15\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 0.0075 - accuracy: 0.9981\n",
      "Epoch 15/15\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 0.0069 - accuracy: 0.9986\n",
      "141/141 [==============================] - 2s 5ms/step - loss: 0.0555 - accuracy: 0.9894\n",
      "test loss, test acc: [0.05552755296230316, 0.9893593192100525]\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 215)]             0         \n",
      "                                                                 \n",
      " multi_category_encoding (Mu  (None, 215)              0         \n",
      " ltiCategoryEncoding)                                            \n",
      "                                                                 \n",
      " normalization (Normalizatio  (None, 215)              431       \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               55296     \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                8224      \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      " classification_head_1 (Acti  (None, 1)                0         \n",
      " vation)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 63,984\n",
      "Trainable params: 63,553\n",
      "Non-trainable params: 431\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "autokeras_model=autoKerasModel('structured_data_classifier/best_model')\n",
    "autokeras_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "autokeras_model.fit(X_train, y_train, epochs=15)\n",
    "results = autokeras_model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"test loss, test acc:\", results)\n",
    "print(autokeras_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c7946d",
   "metadata": {},
   "source": [
    "# Ensemble Learning - Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c635b5e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def majority_voting(y_preds, y_test):\n",
    "    assert y_preds.shape[0] == len(y_test), \"y_preds's length is: {} while y_test's length is: {}. They should be equal.\".format(y_preds.shape[0],len(y_test))\n",
    "    y_pred_vote = []\n",
    "    for preds in y_preds:\n",
    "        if sum(preds) >= 3:\n",
    "            y_pred_vote.append(1)\n",
    "        else:\n",
    "            y_pred_vote.append(0)\n",
    "    #compute accuracy_score\n",
    "    accuracy = acc(y_test, y_pred_vote)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6d69997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9909111061848814\n"
     ]
    }
   ],
   "source": [
    "y_preds = np.ndarray(shape=(5,len(y_test)))\n",
    "i=0\n",
    "for classifier_pair in classifiers.items():\n",
    "    classifier = classifier_pair[1]\n",
    "    y_preds[i] = classifier.predict(X_test)\n",
    "    i += 1\n",
    "y_preds[i] = np.transpose(autokeras_model.predict(X_test, verbose=0))\n",
    "y_preds = np.transpose(y_preds)\n",
    "print('accuracy', majority_voting(y_preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac6377",
   "metadata": {},
   "source": [
    "# Label Flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d765dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_label_flipping(X_train, X_test, y_train, y_test, per, classifier_name, classifier):\n",
    "    flipped_data = specific_label_flipping(y_train, per, 1)\n",
    "    accuracy = 0\n",
    "    if classifier_name == \"AutoKeras\":\n",
    "        classifier.fit(X_train, flipped_data, verbose=0, epochs=15)\n",
    "        y_pred = np.transpose(classifier.predict(X_test, verbose=0))\n",
    "        accuracy = classifier.evaluate(X_test, y_test, verbose=0)[1]\n",
    "    else:\n",
    "        classifier.fit(X_train, flipped_data)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        accuracy = acc(y_test, y_pred)\n",
    "    return y_pred, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fbc4334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flipping Random\n",
    "def random_label_flipping(y_train, per):\n",
    "    flip_count = int(per*(len(y_train)))\n",
    "    flipped_data = copy.deepcopy(y_train)\n",
    "    indices = random.sample(range(len(flipped_data)), flip_count)\n",
    "    for j in indices:\n",
    "        flipped_data[j] = (flipped_data[j] + 1)%2\n",
    "    return flipped_data\n",
    "\n",
    "#Flipping Specific\n",
    "def specific_label_flipping(y_train, per, target):\n",
    "    flipped_data = copy.deepcopy(y_train)\n",
    "    possible_indices = []\n",
    "    for i in range(len(y_train)):\n",
    "        if y_train[i] == target:\n",
    "            possible_indices.append(i)\n",
    "    flip_count = int(per*(len(possible_indices)))\n",
    "    indices = random.sample(possible_indices, flip_count)\n",
    "    for j in indices:\n",
    "        flipped_data[j] = (flipped_data[j] + 1)%2\n",
    "    return flipped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ddbcce5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Trial #1 is starting...\n",
      "Trial #1 is completed with accuracy: 0.97251163821769\n",
      "---------------------------\n",
      "Trial #2 is starting...\n",
      "Trial #2 is completed with accuracy: 0.97628020394591\n",
      "---------------------------\n",
      "Trial #3 is starting...\n",
      "Trial #3 is completed with accuracy: 0.9711815561959655\n",
      "---------------------------\n",
      "Trial #4 is starting...\n",
      "Trial #4 is completed with accuracy: 0.9729549988915983\n",
      "---------------------------\n",
      "Trial #5 is starting...\n",
      "Trial #5 is completed with accuracy: 0.9771669252937264\n",
      "---------------------------\n",
      "Random Poisoning - 0.2\n",
      "SVM's average accuracy: 0.9626246951895366\n",
      "MLP's average accuracy: 0.9632454001330082\n",
      "XGB's average accuracy: 0.9724673021502992\n",
      "LGBM's average accuracy: 0.976989581024163\n",
      "AutoKeras's average accuracy: 0.9781866669654846\n",
      "Average ensemble accuracy: 0.9740190645089781\n"
     ]
    }
   ],
   "source": [
    "percentages = [0.01, 0.05, 0.1, 0.2]\n",
    "per = percentages[3]\n",
    "poisoned_accuracies = {\"SVM\": [], \"MLP\": [], \"XGB\": [], \"LGBM\": [], \"AutoKeras\": []}\n",
    "ensemble_accuracies = []\n",
    "\n",
    "print(\"---------------------------\")\n",
    "for i in range(5):\n",
    "    print(\"Trial #{} is starting...\".format(i+1))\n",
    "    \n",
    "    poisoned_classifiers = {\"SVM\": SVC(kernel = 'linear', degree=3), \"MLP\": MLPClassifier(random_state=1, max_iter=300), \"XGB\": XGBClassifier(), \"LGBM\": LGBMClassifier()} \n",
    "    poisoned_autokeras_model=autoKerasModel('structured_data_classifier/best_model')\n",
    "    poisoned_autokeras_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    poisoned_classifiers[\"AutoKeras\"] = poisoned_autokeras_model\n",
    "    \n",
    "    y_preds = np.ndarray(shape=(len(poisoned_classifiers),len(y_test)))\n",
    "    j=0\n",
    "    for classifier_pair in poisoned_classifiers.items():\n",
    "        poisoned_y_pred, poisoned_accuracy = attack_label_flipping(X_train, X_test, y_train, y_test, per, classifier_pair[0], classifier_pair[1])\n",
    "        y_preds[j] = poisoned_y_pred\n",
    "        j+=1\n",
    "        poisoned_accuracies[classifier_pair[0]].append(poisoned_accuracy)\n",
    "    y_preds = np.transpose(y_preds)\n",
    "    \n",
    "    ensemble_accuracy = majority_voting(y_preds, y_test)\n",
    "    ensemble_accuracies.append(ensemble_accuracy)\n",
    "    \n",
    "    print(\"Trial #{} is completed with accuracy: {}\".format(i+1, ensemble_accuracy))\n",
    "    print(\"---------------------------\")\n",
    "\n",
    "print(\"Random Poisoning - {}\".format(per))\n",
    "for classifier_pair in poisoned_accuracies.items():\n",
    "    accuracies = classifier_pair[1]\n",
    "    print(\"{}'s average accuracy: {}\".format(classifier_pair[0], sum(accuracies)/len(accuracies)))\n",
    "print(\"Average ensemble accuracy:\", sum(ensemble_accuracies)/len(ensemble_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de53925",
   "metadata": {},
   "source": [
    "# SECML Poisoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c61ab9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of c_classifier_LR...\n",
      "Training of c_classifier_LR is done!\n"
     ]
    }
   ],
   "source": [
    "from secml.ml.classifiers.sklearn.c_classifier_logistic import CClassifierLogistic\n",
    "from secml.adv.attacks.poisoning.c_attack_poisoning_logistic_regression import CAttackPoisoningLogisticRegression\n",
    "from secml.data.c_dataset import CDataset\n",
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "\n",
    "lob, ub = 0., 1.  # Bounds of the attack space. Can be set to `None` for unbounded\n",
    "\n",
    "# Should be chosen depending on the optimization problem\n",
    "solver_params = {\n",
    "    'eta': 0.25,\n",
    "    'eta_min': 2.0,\n",
    "    'eta_max': None,\n",
    "    'max_iter': 100,\n",
    "    'eps': 1e-6\n",
    "}\n",
    "\n",
    "dataset = CDataset(X_train, y_train)\n",
    "metric = CMetricAccuracy()\n",
    "\n",
    "# train SVM in the dual space, on a linear kernel, as needed for poisoning\n",
    "c_classifier_LR = CClassifierLogistic()\n",
    "print(\"Training of c_classifier_LR...\")\n",
    "c_classifier_LR.fit(X_train, y_train)\n",
    "print(\"Training of c_classifier_LR is done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9be3011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack started...\n",
      "Attack complete!\n"
     ]
    }
   ],
   "source": [
    "poisoning_strength = 0.1 # Percentage of points to poison in training data\n",
    "n_poisoning_points = 2  # Number of poisoning points to generate\n",
    "\n",
    "pois_attack = CAttackPoisoningLogisticRegression(classifier=c_classifier_LR,\n",
    "                                training_data=dataset,\n",
    "                                val=dataset,\n",
    "                                lb=lob, \n",
    "                                ub=ub,\n",
    "                                solver_params=solver_params)\n",
    "pois_attack.n_points = n_poisoning_points\n",
    "\n",
    "# Run the poisoning attack\n",
    "print(\"Attack started...\")\n",
    "pois_y_pred, _, pois_points_ds, _ = pois_attack.run(X_test, y_test)\n",
    "print(\"Attack complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f4938",
   "metadata": {},
   "source": [
    "# Reading Poisoned data & adding to/replacing with training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27d40aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "pois_points_path = 'poisoning-20.csv'\n",
    "# Importing the dataset\n",
    "pois_points_ds = pd.read_csv(pois_points_path)\n",
    "pois_points_X = pois_points_ds.iloc[:,range(0,pois_points_ds.shape[1]-1)].values\n",
    "pois_points_y = pois_points_ds.iloc[:, -1].values\n",
    "\n",
    "pois_points_y = lb.fit_transform(pois_points_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65b8fab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Trial #1 is starting...\n",
      "Trial #1 is completed with accuracy: 0.9893593438262026\n",
      "---------------------------\n",
      "SECML Poisoning - 10%\n",
      "SVM's average accuracy: 0.9687430724894702\n",
      "MLP's average accuracy: 0.9824872533806251\n",
      "XGB's average accuracy: 0.9886943028153403\n",
      "LGBM's average accuracy: 0.9895810241631567\n",
      "AutoKeras's average accuracy: 0.9895810484886169\n",
      "Average ensemble accuracy: 0.9893593438262026\n"
     ]
    }
   ],
   "source": [
    "poisoned_accuracies = {\"SVM\": [], \"MLP\": [], \"XGB\": [], \"LGBM\": [], \"AutoKeras\": []}\n",
    "ensemble_accuracies = []\n",
    "print(\"---------------------------\")\n",
    "for trial in range(5):\n",
    "    pois_X_train = copy.deepcopy(X_train)\n",
    "    pois_y_train = copy.deepcopy(y_train)\n",
    "\n",
    "    #Uncomment below to RANDOMLY REPLACE with poisoned data\n",
    "    to_be_replaced = random.sample(range(len(pois_X_train)), len(pois_points_X))\n",
    "    new_data = 0\n",
    "    for index in to_be_replaced:\n",
    "        pois_X_train[index] = pois_points_X[new_data]\n",
    "        pois_y_train[index] = pois_points_y[new_data]\n",
    "        new_data += 1\n",
    "\n",
    "    #Uncomment below to ADD poisoned data\n",
    "    \"\"\"pois_X_train = np.append(pois_X_train, pois_points_X, axis=0)\n",
    "    pois_y_train = np.append(pois_y_train, pois_points_y)\"\"\"\n",
    "\n",
    "    print(\"Trial #{} is starting...\".format(trial+1))\n",
    "    \n",
    "    classifiers_secml = {\"SVM\": SVC(kernel = 'linear', degree=3), \"MLP\": MLPClassifier(random_state=1, max_iter=300), \"XGB\": XGBClassifier(), \"LGBM\": LGBMClassifier()} \n",
    "    \n",
    "    y_preds = np.ndarray(shape=(5,len(y_test)))\n",
    "    j=0\n",
    "    for classifier_pair in classifiers_secml.items():\n",
    "        classifier = classifier_pair[1]\n",
    "        classifier.fit(pois_X_train, pois_y_train)\n",
    "        # Predicting the Test set results\n",
    "        poisoned_y_pred = classifier.predict(X_test)\n",
    "        y_preds[j] = poisoned_y_pred\n",
    "        #compute accuracy_score\n",
    "        poisoned_accuracy = acc(y_test, poisoned_y_pred)\n",
    "        poisoned_accuracies[classifier_pair[0]].append(poisoned_accuracy)\n",
    "        j+=1\n",
    "        \n",
    "    ak_model_secml=autoKerasModel('structured_data_classifier/best_model')\n",
    "    ak_model_secml.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    ak_model_secml.fit(pois_X_train, pois_y_train, epochs=15, verbose=0)\n",
    "    y_preds[j] = np.transpose(ak_model_secml.predict(X_test, verbose=0))\n",
    "    poisoned_accuracies[\"AutoKeras\"].append(ak_model_secml.evaluate(X_test, y_test, verbose=0)[1])\n",
    "\n",
    "    y_preds = np.transpose(y_preds)\n",
    "    \n",
    "    ensemble_accuracy = majority_voting(y_preds, y_test)\n",
    "    ensemble_accuracies.append(ensemble_accuracy)\n",
    "    \n",
    "    print(\"Trial #{} is completed with accuracy: {}\".format(trial+1, ensemble_accuracy))\n",
    "    print(\"---------------------------\")\n",
    "\n",
    "print(\"SECML Poisoning - 20%\")\n",
    "for classifier_pair in poisoned_accuracies.items():\n",
    "    accuracies = classifier_pair[1]\n",
    "    print(\"{}'s average accuracy: {}\".format(classifier_pair[0], sum(accuracies)/len(accuracies)))\n",
    "print(\"Average ensemble accuracy:\", sum(ensemble_accuracies)/len(ensemble_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89bee72",
   "metadata": {},
   "source": [
    "# LightGBM - ART Evasion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42379ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 627\n",
      "[LightGBM] [Info] Number of data points in the train set: 10525, number of used features: 209\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[7]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[8]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[9]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[10]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[11]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[12]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[13]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[14]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[15]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[16]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[17]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[18]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[19]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[20]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[21]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[22]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[23]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[24]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[25]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[26]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[27]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[28]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[29]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[30]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[31]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[32]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[33]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[34]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[35]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[36]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[37]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[38]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[39]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[40]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[41]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[42]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[43]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[44]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[45]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[46]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[47]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[48]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[49]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[50]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[51]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[52]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[53]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[54]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[55]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[56]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[57]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[58]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[59]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[60]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[61]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[62]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[63]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[64]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[65]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[66]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[67]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[68]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[69]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[70]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[71]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[72]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[73]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[74]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[75]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[76]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[77]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[78]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[79]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[80]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[81]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[82]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[83]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[84]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[85]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[86]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[87]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[88]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[89]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[90]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[91]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[92]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[93]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[94]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[95]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[96]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[97]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[98]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[99]\tvalid_0's multi_logloss: 0\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[100]\tvalid_0's multi_logloss: 0\n",
      "Accuracy on benign test examples: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ZOO: 100%|███████████████████████████████████████████████████████████████████████| 4511/4511 [2:28:55<00:00,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial test examples: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\"objective\": \"multiclass\", \"metric\": \"multi_logloss\", \"num_class\": 10, \"force_col_wise\": True}\n",
    "train_set = lightgbm.Dataset(X_train, label=np.argmax(y_train, axis=1))\n",
    "test_set = lightgbm.Dataset(X_test, label=np.argmax(y_test, axis=1))\n",
    "lightgbm_trial_model = lightgbm.train(params=params, train_set=train_set, num_boost_round=100, valid_sets=[test_set])\n",
    "\n",
    "classifier = LightGBMClassifier(model=lightgbm_trial_model, clip_values=(0,1))\n",
    "predictions = classifier.predict(X_test)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on benign test examples: {}%\".format(accuracy * 100))\n",
    "\n",
    "\n",
    "attack = ZooAttack(\n",
    "    classifier=classifier,\n",
    "    confidence=0.5,\n",
    "    targeted=False,\n",
    "    learning_rate=1e-1,\n",
    "    max_iter=200,\n",
    "    binary_search_steps=100,\n",
    "    initial_const=1e-1,\n",
    "    abort_early=True,\n",
    "    use_resize=False,\n",
    "    use_importance=False,\n",
    "    nb_parallel=50,\n",
    "    batch_size=1,\n",
    "    variable_h=0.01,\n",
    ")\n",
    "x_test_adv = attack.generate(x=X_test)\n",
    "\n",
    "# Step 7: Evaluate the ART classifier on adversarial test examples\n",
    "\n",
    "predictions = classifier.predict(x_test_adv)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on adversarial test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7c25a1",
   "metadata": {},
   "source": [
    "# LightGBM - ART Poisoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e4b4246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1068\n",
      "[LightGBM] [Info] Number of data points in the train set: 10525, number of used features: 215\n",
      "[LightGBM] [Info] Start training from score -0.520372\n",
      "[LightGBM] [Info] Start training from score -0.902140\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "Accuracy on adversarial test examples: 64.420305918865%\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters for your LightGBM model\n",
    "params = {\"objective\": \"multiclass\", \"metric\": \"multi_logloss\", \"num_class\": 10, \"force_col_wise\": True}\n",
    "\n",
    "# Define your training data and labels\n",
    "train_set = lightgbm.Dataset(X_train, label=np.argmax(y_train, axis=1), free_raw_data=False)\n",
    "test_set = lightgbm.Dataset(X_test, label=np.argmax(y_test, axis=1), free_raw_data=False)\n",
    "\n",
    "# Train the benign LightGBM model\n",
    "lightgbm_trial_model = lightgbm.train(params=params, train_set=train_set, num_boost_round=100, valid_sets=[test_set])\n",
    "\n",
    "# Create a classifier for the benign model\n",
    "classifier = LightGBMClassifier(model=lightgbm_trial_model, clip_values=(0, 1))\n",
    "\n",
    "# Evaluate the benign model on test examples\n",
    "predictions = classifier.predict(X_test)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on benign test examples: {}%\".format(accuracy * 100))\n",
    "\n",
    "# Define the perturbation function for the backdoor attack\n",
    "def trigger_perturbation(x):\n",
    "    poisoned_x = np.copy(x)\n",
    "    backdoor_percentage = 0.05  # % of data that is modified\n",
    "    num_backdoor_points = int(len(x) * backdoor_percentage)\n",
    "    backdoor_indices = np.random.choice(len(x), num_backdoor_points, replace=False)\n",
    "    for idx in backdoor_indices:\n",
    "        poisoned_x[idx] += 0.1\n",
    "    return poisoned_x\n",
    "\n",
    "# Initialize the PoisoningAttackBackdoor with the perturbation function\n",
    "backdoor_attack = PoisoningAttackBackdoor(perturbation=trigger_perturbation)\n",
    "\n",
    "# Use the backdoor_attack to poison the training data\n",
    "target_labels = np.copy(y_train)\n",
    "poisoned_data, _ = backdoor_attack.poison(x=X_train, y=target_labels)\n",
    "\n",
    "# Create a boolean mask to identify the poisoned instances\n",
    "poisoned_mask = (poisoned_data != X_train).any(axis=1)\n",
    "\n",
    "# Set the labels of the poisoned instances to 1\n",
    "target_labels[poisoned_mask] = 1\n",
    "\n",
    "# Train the model on the poisoned data\n",
    "poisoned_train_set = lightgbm.Dataset(poisoned_data, label=target_labels)\n",
    "poisoned_lightgbm_model = lightgbm.train(params=params, train_set=poisoned_train_set, num_boost_round=100, valid_sets=[test_set])\n",
    "\n",
    "# Create a classifier using the poisoned model\n",
    "poisoned_classifier = LightGBMClassifier(model=poisoned_lightgbm_model, clip_values=(0, 1))\n",
    "\n",
    "# Evaluate the poisoned model on adversarial test examples\n",
    "predictions = poisoned_classifier.predict(X_test)  # Use the adversarial test data\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on adversarial test examples: {}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762f1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
