{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2be1ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "#from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import f1_score as f1\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64654307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93d440",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def2d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "krono_data_path1 = 'data/kronodroid.csv'\n",
    "# Importing the dataset\n",
    "Krono_data = pd.read_csv(krono_data_path1)\n",
    "Krono_data = Krono_data.sample(frac = 1)\n",
    "X = Krono_data.iloc[:,range(1,Krono_data.shape[1]-1)].values\n",
    "y = Krono_data.iloc[:, -1].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.astype(int), test_size = 0.3, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1699ad50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "RFC\n",
      "accuracy 0.9583653271905127\n",
      "---------------------------\n",
      "SVM\n",
      "accuracy 0.909180104086682\n",
      "---------------------------\n",
      "SGD\n",
      "accuracy 0.9371640644996161\n",
      "---------------------------\n",
      "XGB\n",
      "accuracy 0.9484685607030117\n",
      "---------------------------\n",
      "LGBM\n",
      "accuracy 0.9561044279498336\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "classifiers = {\"RFC\": RandomForestClassifier(), \"SVM\": SVC(kernel = 'poly', degree=3), \"SGD\": SGDClassifier(), \"XGB\": XGBClassifier(), \"LGBM\": LGBMClassifier()} \n",
    "\n",
    "for classifier_pair in classifiers.items():\n",
    "    print(\"---------------------------\")\n",
    "    print(classifier_pair[0])\n",
    "    classifier = classifier_pair[1]\n",
    "    classifier.fit(X_train, y_train)\n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    #compute accuracy_score\n",
    "    accuracy = acc(y_test, y_pred)\n",
    "    print('accuracy', accuracy)\n",
    "    \n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45710eb",
   "metadata": {},
   "source": [
    "# Ensemble Learning - Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d49449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_voting(y_preds, y_test):\n",
    "    assert y_preds.shape[0] == len(y_test), \"y_preds's length is: {} while y_test's length is: {}. They should be equal.\".format(y_preds.shape[0],len(y_test))\n",
    "    y_pred_vote = []\n",
    "    for preds in y_preds:\n",
    "        if sum(preds) >= 3:\n",
    "            y_pred_vote.append(1)\n",
    "        else:\n",
    "            y_pred_vote.append(0)\n",
    "    #compute accuracy_score\n",
    "    accuracy = acc(y_test, y_pred_vote)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6163769b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifiers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m y_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;28mlen\u001b[39m(y_test)))\n\u001b[0;32m      2\u001b[0m i\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m classifier_pair \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclassifiers\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      4\u001b[0m     classifier \u001b[38;5;241m=\u001b[39m classifier_pair[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      5\u001b[0m     y_preds[i] \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifiers' is not defined"
     ]
    }
   ],
   "source": [
    "y_preds = np.ndarray(shape=(5,len(y_test)))\n",
    "i=0\n",
    "for classifier_pair in classifiers.items():\n",
    "    classifier = classifier_pair[1]\n",
    "    y_preds[i] = classifier.predict(X_test)\n",
    "    i += 1\n",
    "y_preds = np.transpose(y_preds)\n",
    "print('accuracy', majority_voting(y_preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8bd8e2",
   "metadata": {},
   "source": [
    "# Label Flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae2da166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_label_flipping(X_train, X_test, y_train, y_test, per, classifier):\n",
    "    flipped_data = specific_label_flipping(y_train, per, 1)\n",
    "    classifier.fit(X_train, flipped_data)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    return y_pred, acc(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8929ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flipping Random\n",
    "def random_label_flipping(y_train, per):\n",
    "    flip_count = int(per*(len(y_train)))\n",
    "    flipped_data = copy.deepcopy(y_train)\n",
    "    indices = random.sample(range(len(flipped_data)), flip_count)\n",
    "    for j in indices:\n",
    "        flipped_data[j] = (flipped_data[j] + 1)%2\n",
    "    return flipped_data\n",
    "\n",
    "#Flipping Specific\n",
    "def specific_label_flipping(y_train, per, target):\n",
    "    flipped_data = copy.deepcopy(y_train)\n",
    "    possible_indices = []\n",
    "    for i in range(len(y_train)):\n",
    "        if y_train[i] == target:\n",
    "            possible_indices.append(i)\n",
    "    flip_count = int(per*(len(possible_indices)))\n",
    "    indices = random.sample(possible_indices, flip_count)\n",
    "    for j in indices:\n",
    "        flipped_data[j] = (flipped_data[j] + 1)%2\n",
    "    return flipped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a26f8720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Trial #1 is starting...\n",
      "Trial #1 is completed with accuracy: 0.9430935926968689\n",
      "---------------------------\n",
      "Trial #2 is starting...\n",
      "Trial #2 is completed with accuracy: 0.9404061086937975\n",
      "---------------------------\n",
      "Trial #3 is starting...\n",
      "Trial #3 is completed with accuracy: 0.9473167818445525\n",
      "---------------------------\n",
      "Trial #4 is starting...\n",
      "Trial #4 is completed with accuracy: 0.9472314648920741\n",
      "---------------------------\n",
      "Trial #5 is starting...\n",
      "Trial #5 is completed with accuracy: 0.9446719563177204\n",
      "---------------------------\n",
      "Random Poisoning - 0.2\n",
      "RFC's average accuracy: 0.9245286238375566\n",
      "SVM's average accuracy: 0.8422660182578279\n",
      "SGD's average accuracy: 0.8896339902738675\n",
      "XGB's average accuracy: 0.9248869550379659\n",
      "LGBM's average accuracy: 0.9502687484003071\n",
      "Average ensemble accuracy: 0.9445439808890027\n"
     ]
    }
   ],
   "source": [
    "percentages = [0.01, 0.05, 0.1, 0.2]\n",
    "per = percentages[3]\n",
    "poisoned_accuracies = {\"RFC\": [], \"SVM\": [], \"SGD\": [], \"XGB\": [], \"LGBM\": []} \n",
    "ensemble_accuracies = []\n",
    "\n",
    "print(\"---------------------------\")\n",
    "for i in range(5):\n",
    "    print(\"Trial #{} is starting...\".format(i+1))\n",
    "    \n",
    "    poisoned_classifiers = {\"RFC\": RandomForestClassifier(), \"SVM\": SVC(kernel = 'poly', degree=3), \"SGD\": SGDClassifier(), \"XGB\": XGBClassifier(), \"LGBM\": LGBMClassifier()} \n",
    "       \n",
    "    y_preds = np.ndarray(shape=(len(poisoned_classifiers),len(y_test)))\n",
    "    j=0\n",
    "    for classifier_pair in poisoned_classifiers.items():\n",
    "        poisoned_y_pred, poisoned_accuracy = attack_label_flipping(X_train, X_test, y_train, y_test, per, classifier_pair[1])\n",
    "        y_preds[j] = poisoned_y_pred\n",
    "        j+=1\n",
    "        poisoned_accuracies[classifier_pair[0]].append(poisoned_accuracy)    \n",
    "    y_preds = np.transpose(y_preds)\n",
    "    \n",
    "    ensemble_accuracy = majority_voting(y_preds, y_test)\n",
    "    ensemble_accuracies.append(ensemble_accuracy)\n",
    "    \n",
    "    print(\"Trial #{} is completed with accuracy: {}\".format(i+1, ensemble_accuracy))\n",
    "    print(\"---------------------------\")\n",
    "\n",
    "print(\"Random Poisoning - {}\".format(per))\n",
    "for classifier_pair in poisoned_accuracies.items():\n",
    "    accuracies = classifier_pair[1]\n",
    "    print(\"{}'s average accuracy: {}\".format(classifier_pair[0], sum(accuracies)/len(accuracies)))\n",
    "print(\"Average ensemble accuracy:\", sum(ensemble_accuracies)/len(ensemble_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f11dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
