{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ff535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3870ed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "#from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "import keras\n",
    "import autokeras as ak\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "025a9cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134435, 2381) (134435,)\n",
      "(94104, 2381) (40331, 2381)\n"
     ]
    }
   ],
   "source": [
    "filename = 'bodmas.npz'\n",
    "data = np.load('./' + filename)\n",
    "X = data['X']  # all the feature vectors\n",
    "y = data['y']  # labels, 0 as benign, 1 as malicious\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.astype(int), test_size = 0.3, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b4d1e",
   "metadata": {},
   "source": [
    "# Train-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8907912",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "---------------------------\n",
      "GaussianNB\n",
      "Confusion Matrix [[10293 12751]\n",
      " [  516 16771]]\n",
      "accuracy 0.671047085368575\n",
      "precision 0.671047085368575\n",
      "recall 0.9701509805055822\n",
      "f1 0.7165715994787327\n",
      "---------------------------\n",
      "RFC\n",
      "Confusion Matrix [[23006    38]\n",
      " [  181 17106]]\n",
      "accuracy 0.994569933797823\n",
      "precision 0.994569933797823\n",
      "recall 0.9895297044021519\n",
      "f1 0.9936394528186809\n",
      "---------------------------\n",
      "SVM\n",
      "Confusion Matrix [[22581   463]\n",
      " [  327 16960]]\n",
      "accuracy 0.9804120899556172\n",
      "precision 0.9804120899556172\n",
      "recall 0.9810840515994678\n",
      "f1 0.9772399884759435\n",
      "---------------------------\n",
      "DT\n",
      "Confusion Matrix [[22783   261]\n",
      " [  196 17091]]\n",
      "accuracy 0.9886687659616672\n",
      "precision 0.9886687659616672\n",
      "recall 0.9886620003470816\n",
      "f1 0.9868067784866769\n",
      "---------------------------\n",
      "SGD\n",
      "Confusion Matrix [[22826   218]\n",
      " [  364 16923]]\n",
      "accuracy 0.9855694131065433\n",
      "precision 0.9855694131065433\n",
      "recall 0.9789437149302944\n",
      "f1 0.983095155106309\n",
      "---------------------------\n",
      "MLP\n",
      "Confusion Matrix [[22933   111]\n",
      " [  106 17181]]\n",
      "accuracy 0.994619523443505\n",
      "precision 0.994619523443505\n",
      "recall 0.9938682246775034\n",
      "f1 0.9937245148789727\n",
      "---------------------------\n",
      "XGB\n",
      "Confusion Matrix [[23020    24]\n",
      " [   80 17207]]\n",
      "accuracy 0.997421338424537\n",
      "precision 0.997421338424537\n",
      "recall 0.9953722450396252\n",
      "f1 0.9969870792050525\n",
      "---------------------------\n",
      "LGBM\n",
      "Confusion Matrix [[22993    51]\n",
      " [   88 17199]]\n",
      "accuracy 0.9965535196251023\n",
      "precision 0.9965535196251023\n",
      "recall 0.9949094695435877\n",
      "f1 0.9959753308046443\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "classifiers = {\"SGD\": SGDClassifier(),\"MLP\": MLPClassifier(random_state=1, max_iter=300), \"XGB\": XGBClassifier(), \"LGBM\": LGBMClassifier()} \n",
    "\n",
    "print('Starting training...')\n",
    "\n",
    "for classifier_pair in classifiers.items():\n",
    "    print(\"---------------------------\")\n",
    "    print(classifier_pair[0])\n",
    "    \n",
    "    classifier = classifier_pair[1]\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    #compute accuracy_score\n",
    "    accuracy = acc(y_test, y_pred)\n",
    "    print('accuracy', accuracy)\n",
    "    \n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c4dcab",
   "metadata": {},
   "source": [
    "# AutoKeras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b106b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model=keras.models.load_model('structured_data_classifier/best_model', compile=False)\n",
    "print(saved_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27fdafbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m custom_objects \u001b[38;5;241m=\u001b[39m ak\u001b[38;5;241m.\u001b[39mCUSTOM_OBJECTS\n\u001b[1;32m----> 3\u001b[0m saved_model_ak\u001b[38;5;241m=\u001b[39m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstructured_data_classifier/best_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(saved_model_ak\u001b[38;5;241m.\u001b[39msummary())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:115\u001b[0m, in \u001b[0;36m_BaseOptimizer._process_kwargs\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated in `optimizer_experimental.Optimizer`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, please check the docstring for valid arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    112\u001b[0m         k,\n\u001b[0;32m    113\u001b[0m     )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid argument, kwargs should be empty \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for `optimizer_experimental.Optimizer`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`."
     ]
    }
   ],
   "source": [
    "custom_objects = ak.CUSTOM_OBJECTS\n",
    "\n",
    "saved_model_ak=keras.models.load_model('structured_data_classifier/best_model', custom_objects=custom_objects)\n",
    "print(saved_model_ak.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f802fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoKerasModel(path_best_model):\n",
    "    saved_model = keras.models.load_model(path_best_model, compile=False)\n",
    "    input_layer = Input(shape=(2381,))\n",
    "    x = saved_model.layers[1](input_layer)\n",
    "    x = saved_model.layers[2](x)\n",
    "    x = Dense(units=32)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dense(units=32)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dense(units=1)(x)\n",
    "    x = saved_model.layers[-1](x)\n",
    "    new_model = Model(inputs=input_layer, outputs=x)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34c1b4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 2381)]            0         \n",
      "                                                                 \n",
      " multi_category_encoding (Mu  (None, 2381)             0         \n",
      " ltiCategoryEncoding)                                            \n",
      "                                                                 \n",
      " normalization (Normalizatio  (None, 2381)             4763      \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                76224     \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      " classification_head_1 (Acti  (None, 1)                0         \n",
      " vation)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,076\n",
      "Trainable params: 77,313\n",
      "Non-trainable params: 4,763\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "autokeras_model=autoKerasModel('structured_data_classifier/best_model')\n",
    "autokeras_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\"\"\"autokeras_model.fit(X_train, y_train, epochs=15)\n",
    "results = autokeras_model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"test loss, test acc:\", results)\"\"\"\n",
    "print(autokeras_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e7b9d",
   "metadata": {},
   "source": [
    "# Ensemble Learning - Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a592b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_voting(y_preds, y_test):\n",
    "    assert y_preds.shape[0] == len(y_test), \"y_preds's length is: {} while y_test's length is: {}. They should be equal.\".format(y_preds.shape[0],len(y_test))\n",
    "    y_pred_vote = []\n",
    "    for preds in y_preds:\n",
    "        if sum(preds) >= 3:\n",
    "            y_pred_vote.append(1)\n",
    "        else:\n",
    "            y_pred_vote.append(0)\n",
    "    #compute accuracy_score\n",
    "    accuracy = acc(y_test, y_pred_vote)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bce0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.ndarray(shape=(5,len(y_test)))\n",
    "i=0\n",
    "for classifier_pair in classifiers.items():\n",
    "    classifier = classifier_pair[1]\n",
    "    y_preds[i] = classifier.predict(X_test)\n",
    "    i += 1\n",
    "y_preds[i] = np.transpose(autokeras_model.predict(X_test, verbose=0))\n",
    "y_preds = np.transpose(y_preds)\n",
    "print('accuracy', majority_voting(y_preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5927c81e",
   "metadata": {},
   "source": [
    "# Label Flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec9550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_label_flipping(X_train, X_test, y_train, y_test, per, classifier_name, classifier):\n",
    "    flipped_data = random_label_flipping(y_train, per)\n",
    "    accuracy = 0\n",
    "    if classifier_name == \"AutoKeras\":\n",
    "        classifier.fit(X_train, flipped_data, verbose=0, epochs=15)\n",
    "        y_pred = np.transpose(classifier.predict(X_test, verbose=0))\n",
    "        accuracy = classifier.evaluate(X_test, y_test, verbose=0)[1]\n",
    "    else:\n",
    "        classifier.fit(X_train, flipped_data)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        accuracy = acc(y_test, y_pred)\n",
    "    return y_pred, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60acab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flipping Random\n",
    "def random_label_flipping(y_train, per):\n",
    "    flip_count = int(per*(len(y_train)))\n",
    "    flipped_data = copy.deepcopy(y_train)\n",
    "    indices = random.sample(range(len(flipped_data)), flip_count)\n",
    "    for j in indices:\n",
    "        flipped_data[j] = (flipped_data[j] + 1)%2\n",
    "    return flipped_data\n",
    "\n",
    "#Flipping Specific\n",
    "def specific_label_flipping(y_train, per, target):\n",
    "    flipped_data = copy.deepcopy(y_train)\n",
    "    possible_indices = []\n",
    "    for i in range(len(y_train)):\n",
    "        if y_train[i] == target:\n",
    "            possible_indices.append(i)\n",
    "    flip_count = int(per*(len(possible_indices)))\n",
    "    indices = random.sample(possible_indices, flip_count)\n",
    "    for j in indices:\n",
    "        flipped_data[j] = (flipped_data[j] + 1)%2\n",
    "    return flipped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8691e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = [0.01, 0.05, 0.1, 0.2]\n",
    "\n",
    "print(\"---------------------------\")\n",
    "for per in percentages:    \n",
    "    poisoned_accuracies = {\"SGD\": [], \"MLP\": [], \"XGB\": [], \"LGBM\": [], \"AutoKeras\": []}\n",
    "    ensemble_accuracies = []\n",
    "    print( per, \"poisoning starting...\")\n",
    "    for i in range(5):\n",
    "        print(\"Trial #{} is starting...\".format(i+1))\n",
    "        \n",
    "        poisoned_classifiers = {\"SGD\": SGDClassifier(), \"MLP\": MLPClassifier(random_state=1, max_iter=300), \"XGB\": XGBClassifier(), \"LGBM\": LGBMClassifier()} \n",
    "        poisoned_autokeras_model=autoKerasModel('structured_data_classifier/best_model')\n",
    "        poisoned_autokeras_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        poisoned_classifiers[\"AutoKeras\"] = poisoned_autokeras_model\n",
    "        \n",
    "        y_preds = np.ndarray(shape=(len(poisoned_classifiers),len(y_test)))\n",
    "        j=0\n",
    "        for classifier_pair in poisoned_classifiers.items():\n",
    "            poisoned_y_pred, poisoned_accuracy = attack_label_flipping(X_train, X_test, y_train, y_test, per, classifier_pair[0], classifier_pair[1])\n",
    "            y_preds[j] = poisoned_y_pred\n",
    "            j+=1\n",
    "            poisoned_accuracies[classifier_pair[0]].append(poisoned_accuracy)\n",
    "        y_preds = np.transpose(y_preds)\n",
    "        \n",
    "        ensemble_accuracy = majority_voting(y_preds, y_test)\n",
    "        ensemble_accuracies.append(ensemble_accuracy)\n",
    "        \n",
    "        print(\"Trial #{} is completed with accuracy: {}\".format(i+1, ensemble_accuracy))\n",
    "        print(\"---------------------------\")\n",
    "\n",
    "    print(\"Random Poisoning - {}\".format(per))\n",
    "    for classifier_pair in poisoned_accuracies.items():\n",
    "        accuracies = classifier_pair[1]\n",
    "        print(\"{}'s average accuracy: {}\".format(classifier_pair[0], sum(accuracies)/len(accuracies)))\n",
    "    print(\"Average ensemble accuracy:\", sum(ensemble_accuracies)/len(ensemble_accuracies))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
